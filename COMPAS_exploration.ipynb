{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GabrielleRab/SRMPmachine/blob/main/COMPAS_exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JJgtSBRa5ka"
      },
      "source": [
        "#Understanding Bias in AI Used in the Criminal Justice System\n",
        "\n",
        "\n",
        "Adapted from a notebook created by [Trenton Chang](https://trentonchang.org) (ctrenton@umich.edu) and Daniela Ganelin (daniela.inspiritai@gmail.com), [Inspirit AI](https://www.inspiritai.com/)\n",
        "\n",
        "*This notebook may be modified or reproduced only for non-commercial educational purposes; this notice must be retained.*\n",
        "\n",
        "In this notebook, we'll examine the role of artificial intelligence (AI) systems in the criminal justice system. We'll:\n",
        "\n",
        "- Learn about the COMPAS model used in the criminal justice system\n",
        "- Use basic machine learning techniques in Python to train a risk assessment model on the COMPAS data\n",
        "- Analyze our model for bias - and discuss techniques for fighting it!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dod45spk3vg7"
      },
      "source": [
        "## Background: What is COMPAS?\n",
        "\n",
        "**[COMPAS](https://en.wikipedia.org/wiki/COMPAS_(software)) is a risk-assessment tool that predicts whether a defendant is likely to commit another offense (*redicidivate*).** It's designed to be used in the [_pre-trial detention phase_](https://en.wikipedia.org/wiki/Remand_(detention)): this is when a judge decides whether someone who's been arrested will stay in jail until their trial or be releaed on bail. Software like COMPAS [is used in many parts of the U.S.](https://dl.acm.org/doi/pdf/10.1145/3022181).\n",
        "\n",
        "**Discuss: What's your gut reaction about a tool like this?** Do you think an algorithm would help or hurt a judge in making fair decisions that protect both public safety and defendants' rights?\n",
        "\n",
        "We'll use data from Broward County, Florida obtained by [ProPublica's audit of COMPAS](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing). Alarmingly, **ProPublica found that COMPAS risk scores are systematically worse for Black defendants.** Opponents of COMPAS see it as a continuation of systemic anti-Blackness in the U.S. justice system; supporters argue this system might be more accurate and less biased than human judges.\n",
        "\n",
        "Today, we'll be building our own risk assessment models, so that we can see where bias might come from - and whether we can fight it. We'll be discussing difficult and sensitive material, so please be mindful of how your words affect other people!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ww6WGIizMA6C"
      },
      "source": [
        "## Data Exploration\n",
        "\n",
        "Let's start by exploring the Broward County dataset!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nMfF63z3sP1",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb3a05e9-fea0-4e7f-e68c-b7060fcbfc28"
      },
      "source": [
        "#@title IMPORTANT: Run this cell to set up our tools and data!\n",
        "\n",
        "%load_ext google.colab.data_table\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "sns.set_theme(style=\"darkgrid\")\n",
        "np.set_printoptions(edgeitems=10)\n",
        "\n",
        "!wget -q https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\n",
        "print ()\n",
        "\n",
        "data = pd.read_csv(\"compas-scores-two-years.csv\", header=0)\n",
        "data = data.drop(labels=['id', 'name', 'first', 'last', 'compas_screening_date', 'dob', 'days_b_screening_arrest',\n",
        "                         'c_jail_in', 'c_jail_out', 'c_case_number', 'c_offense_date', 'c_arrest_date', 'c_days_from_compas',\n",
        "                         'r_case_number', 'r_charge_degree', 'r_days_from_arrest', 'r_offense_date', 'r_charge_desc',\n",
        "                         'r_jail_in', 'r_jail_out', 'vr_case_number', 'vr_charge_degree', 'vr_offense_date', 'decile_score.1',\n",
        "                         'violent_recid', 'vr_charge_desc', 'in_custody', 'out_custody', 'priors_count.1', 'start', 'end',\n",
        "                         'v_screening_date', 'event', 'type_of_assessment', 'v_type_of_assessment', 'screening_date',\n",
        "                         'score_text', 'v_score_text', 'v_decile_score', 'decile_score', 'is_recid', 'is_violent_recid'], axis=1)\n",
        "data.columns = ['sex', 'age', 'age_category', 'race', 'juveline_felony_count', 'juveline_misdemeanor_count', 'juveline_other_count',\n",
        "              'prior_convictions', 'current_charge', 'charge_description', 'recidivated_next_two_years']\n",
        "data['current_charge'] = data['current_charge'].replace({\"F\":\"Felony\",\"M\":\"Misdemeanor\", \"O\":\"Other\"})\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def plot_errors(model,input_data, output_data, title='Confusion matrix'):\n",
        "  pred = model.predict(input_data)\n",
        "  cm = confusion_matrix(output_data, pred)\n",
        "  sns.heatmap(cm, annot=True, cmap=plt.get_cmap('Blues'), fmt='d')\n",
        "  plt.title(title)\n",
        "  plt.ylabel(\"Actual\")\n",
        "  plt.xlabel(\"Predicted (model output)\")\n",
        "  plt.show()\n",
        "\n",
        "print (\"Tools and data loaded!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tools and data loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMXUgStp8W7W"
      },
      "source": [
        "The most important thing to start exploring this problem is to **visualize the data.** Let's print out the dataframe below! This is a selection of the Broward County data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUPeJpUz4g7I"
      },
      "source": [
        "#Click the button to the left to run this code!\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBr9WXmTPs8I"
      },
      "source": [
        "**Discuss:**\n",
        "- What kind of data do we have on the defendants?\n",
        "- Do you notice any patterns?\n",
        "- Is there anything surprising or confusing?\n",
        "- Which of these columns are \"fair\" to consider in predicting whether someone will recidivate? Which are \"unfair\" or potentially harmful?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81PSvm9_sm0p"
      },
      "source": [
        "### Data Visualizations\n",
        "\n",
        "Let's make some visualizations to make sense of our data, using a tool called [Seaborn](https://seaborn.pydata.org/generated/seaborn.countplot.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWZuaEWkboKL"
      },
      "source": [
        "#### Visualizing Sex\n",
        "\n",
        "To start with, **how many defendants do we have of each sex?** Run the code below to make a graph:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgM7brN0smK4"
      },
      "source": [
        "graph = sns.countplot(x=\"sex\", data=data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fOuerH1Qyoq"
      },
      "source": [
        "**Discuss:** How would you intepret these results? Can you explain this pattern?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awHIqk0CbtPh"
      },
      "source": [
        "#### Visualizing Age Category\n",
        "\n",
        "Now, try plotting **the number of defendants in each age category** by adapting the code above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cbi_79SUuKat"
      },
      "source": [
        "graph = sns.countplot(x=\"age_category\", data=data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeOM93XXSeyo"
      },
      "source": [
        "Any interesting patterns here?\n",
        "\n",
        "Next, let's plot the **number of defendants of each race:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm5Hb_W8bz5b"
      },
      "source": [
        "#### Visualizing Race\n",
        "\n",
        "Now, try plotting **the number of defendants in each age category** by adapting the code above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zgSdNmlb7pA"
      },
      "source": [
        "graph = sns.countplot(x=\"race\", data=data)\n",
        "\n",
        "#makes formatting nicer!\n",
        "_ = graph.set_xticklabels(graph.get_xticklabels(), rotation=90)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkNKUyPxS29E"
      },
      "source": [
        "**What do you notice here?** Why might it be hard to learn how well our model works for Asian or Native American defendants?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4s-CUmEcF-Z"
      },
      "source": [
        "#### Visualizing Recidivism\n",
        "\n",
        "\n",
        "Finally, let's make a graph for a particularly important column: **\"recidivated_next_two_years\". This column shows whether the defendant was arrested again in the next two years** after the current arrest.\n",
        "It's what our risk assessment model is trying to predict!\n",
        "\n",
        "\"0\" means \"No, did not recidivate\", and \"1\" means \"Yes, did recidivate.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twX9sYmPZdNn"
      },
      "source": [
        "graph = sns.countplot(x=\"recidivated_next_two_years\", data=data)\n",
        "\n",
        "#makes formatting nicer!\n",
        "_ = graph.set_xticklabels(graph.get_xticklabels(), rotation=90)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWypcxg6cS19"
      },
      "source": [
        "Does the pattern here surprise you?\n",
        "\n",
        "You're welcome to keep exploring the data on your own! [Pandas](https://towardsdatascience.com/data-exploration-101-with-pandas-e059d0661313) and [Seaborn](https://seaborn.pydata.org/) are two useful tools."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wgCyPU-PQlM"
      },
      "source": [
        "## Training a Machine Learning Model\n",
        "\n",
        "Now, let's train a machine learning model using our data!\n",
        "\n",
        "Using machine learning, we'll train a model to predict recidivism.\n",
        "\n",
        "Our model's **input** will be information about defendants' backgrounds.\n",
        "\n",
        "Our model's **output** will be a prediction of whether they **will** (1) or **won't** (0) recidivate.\n",
        "\n",
        "**We'll go through the following steps:**\n",
        "1. Prepare our data by turning it into numbers, then splitting it into **training** and **testing** data\n",
        "2. Set up a model called *logistic regression*\n",
        "3. Train a logistic regression model on the **training** data\n",
        "4. Evaluate our model, or see how well it does, on the **testing** data\n",
        "\n",
        "Let's give it a go!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRTNfdbpX4gx"
      },
      "source": [
        "### Step 1: Prepare our Data\n",
        "\n",
        "There are a few things we'll need to do to prepare our data for machine learning!\n",
        "\n",
        "Computers \"see\" the world in terms of numbers, so we'll need to represent our data that way as well. Let's make that conversion:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjTBtIuFBy4D",
        "cellView": "form"
      },
      "source": [
        "#@title Run this to represent our data numerically!\n",
        "data_num = data.copy()\n",
        "value_counts = data_num['charge_description'].value_counts()\n",
        "data_num = data_num[data_num['charge_description'].isin(value_counts[value_counts >= 70].index)].reset_index(drop=True) # drop rare charges\n",
        "for colname in data_num.select_dtypes(include='object').columns: # use get_dummies repeatedly one-hot encode categorical columns\n",
        "  one_hot = pd.get_dummies(data_num[colname])\n",
        "  data_num = data_num.drop(colname, axis=1)\n",
        "  data_num = data_num.join(one_hot)\n",
        "data_num = pd.concat((data_num,data_num.pop(\"recidivated_next_two_years\")),axis=1)\n",
        "data_num.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ju7NvbCyCI1r"
      },
      "source": [
        "**Discuss:** What's changed? What does a 0 or 1 represent here? Did every possible charge get its own column?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4O9EJfdExup"
      },
      "source": [
        "We also need to select out `input_data` and `output_data`.\n",
        "\n",
        "First, let's choose our `output_data` - We want to look at recidivism"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uruZiVqjEG-e"
      },
      "source": [
        "output_data = data_num[[\"recidivated_next_two_years\"]] #FILL ME IN\n",
        "\n",
        "output_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0Y2fhPuExhb"
      },
      "source": [
        "For our `input_data`, we'll use **all the other columns**. Here's the full list of columns:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlRkjHfDD0rD"
      },
      "source": [
        "data_num.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTDBD9pdFZy9"
      },
      "source": [
        "Based on the above, we will fill in the columns to use as inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycvNL3YYYAuu"
      },
      "source": [
        "input_data = data_num[['age', 'juveline_felony_count', 'juveline_misdemeanor_count',\n",
        "       'juveline_other_count', 'prior_convictions', 'Female', 'Male',\n",
        "       '25 - 45', 'Greater than 45', 'Less than 25', 'African-American',\n",
        "       'Asian', 'Caucasian', 'Hispanic', 'Native American', 'Other', 'Felony',\n",
        "       'Misdemeanor', 'Battery', 'Burglary Conveyance Unoccup',\n",
        "       'Burglary Unoccupied Dwelling', 'DUI Property Damage/Injury',\n",
        "       'Driving Under The Influence', 'Driving While License Revoked',\n",
        "       'Felony Battery (Dom Strang)', 'Felony Driving While Lic Suspd',\n",
        "       'Grand Theft (Motor Vehicle)', 'Grand Theft in the 3rd Degree',\n",
        "       'Pos Cannabis W/Intent Sel/Del', 'Poss3,4 Methylenedioxymethcath',\n",
        "       'Possess Cannabis/20 Grams Or Less', 'Possession of Cannabis',\n",
        "       'Possession of Cocaine', 'arrest case no charge']] #FILL ME IN\n",
        "\n",
        "input_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0E4XWBwGjM6"
      },
      "source": [
        "You should have 34 columns in `input_data`.\n",
        "\n",
        "One final step: let's [divide](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) our data into **training data** that we'll use to train our model, and **testing data** that we'll use to test how well it works on real-world data.\n",
        "\n",
        "Why do we do this? You can think of the **training data** as homework problems (`input_train`) and solutions (`output_train`): as we do more and more homework problems, we can better understand how to solve them. Then the **testing_ ata** is like a test to see whether we generalize our knowledge to solve new problems!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYi9TcZ5KlqE"
      },
      "source": [
        "# You don't need to understand the details of this code!\n",
        "# Check out the link if you're curious.\n",
        "\n",
        "input_train, input_test, output_train, output_test = train_test_split(input_data, output_data, test_size=0.3, random_state = 1)\n",
        "output_train = output_train.to_numpy().ravel() #this is formatting\n",
        "output_test = output_test.to_numpy().ravel() #this is formatting"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGU331WvLDcZ"
      },
      "source": [
        "Try printing any of them out if you're curious!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW5OdgWOYfMg"
      },
      "source": [
        "### Step 2: Set up our Model\n",
        "\n",
        "Now, we need to set up the model (machine learning tool) we'll use.\n",
        "\n",
        "We'll be using a common model called [**logistic regression**](https://machinelearningmastery.com/logistic-regression-for-machine-learning/), a type of model that uses numerical inputs to predict 0/1 outputs. You can think of this as answering a yes/no question: in this case, whether a person will recidivate. You can also explore many other machine learning [classification models here](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)!\n",
        "\n",
        "![linear-regression-vs-logistic-regression.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the code below to create the model."
      ],
      "metadata": {
        "id": "3kZeNMLLVhkn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1nSU0Tlr7Nf"
      },
      "source": [
        "model = LogisticRegression(max_iter=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQ-1SPBQZXh1"
      },
      "source": [
        "### Step 3: Train our Model\n",
        "\n",
        "Now, we'll need to feed our training data into the model and train (or `fit`) it!\n",
        "\n",
        "We'll use `model.fit()` and fill in the `input_train` and `output_train` so our model can learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKEcNpnfxjJG"
      },
      "source": [
        "_ = model.fit(input_train, output_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbyPmT4FaBjj"
      },
      "source": [
        "### Step 4: Evaluate our Model\n",
        "\n",
        "Now, our model is trained to make predictions. Let's see how good our model's predictions are!\n",
        "\n",
        "\n",
        "We'll use `model.predict` and fill in the `input_test`. **Discuss:** Why don't we need to plug in the output here?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIJtFiI-b9DR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bc19054-32cf-4e77-d73c-2e6f4bcb53d6"
      },
      "source": [
        "output_pred = model.predict(input_test)\n",
        "\n",
        "output_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, ..., 1, 0, 1, 0, 0, 0, 0, 1, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3WxlouRRJ5u"
      },
      "source": [
        "For comparison, here are the true answers - this is what our model was trying to predict! How close are they?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wp0-iiDVPP_q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "560fcb77-68e3-4171-e478-ae874f20aaee"
      },
      "source": [
        "output_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 1, 1, 0, 1, 1, 1, 0, ..., 1, 1, 1, 0, 0, 0, 1, 1, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3i_nJQPRZAI"
      },
      "source": [
        "We can use `accuracy_score` to get our overall accuracy, or the **percent of predictions that are correct.** To do this, we'll compare the **true answers** (`output_test`) and **predicted answers** (`output_pred`). How'd we do?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3qqytQKReGE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e84be6af-c598-49c1-f45e-8978c2baf0c1"
      },
      "source": [
        "accuracy_score(output_test, output_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.675"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHWHwa_FSKSo"
      },
      "source": [
        "What's your accuracy as a percent?\n",
        "\n",
        "**Discuss:** Is that high enough to use in practice? Do you think this is better or worse than a human judge could do at predicting future recidivism?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMDGPcWgRRuF"
      },
      "source": [
        "## Evaluating Bias\n",
        "\n",
        "Now we have a model, and we know its overall accuracy.\n",
        "\n",
        "But is our model **biased**? Does it perform better for some racial groups than others?\n",
        "\n",
        "Let's check it out!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMU5YbzfN6RE"
      },
      "source": [
        "### Comparing Accuracy\n",
        "We're going to compare the African-American and Caucasian groups, because that's where we have enough data points to draw conclusions. (What are we missing by only focusing on those two groups?)\n",
        "\n",
        "Let's see how accurate our model is for **only African-Americans** vs. **only Caucasians**. Is it about the same?\n",
        "\n",
        "We'll do some fancy [indexing](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html) to separate our test datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Y3Z5BpJqZ3e"
      },
      "source": [
        "input_test_AA = input_test[input_test['African-American'] == 1]\n",
        "output_test_AA = output_test[input_test['African-American'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe5WLIHhVVcV"
      },
      "source": [
        "Let's find our **accuracy for only African-Americans**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBkULzsIVUd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc790e9f-01a5-4ccf-dcf5-147cf9a90eb0"
      },
      "source": [
        "output_pred_AA = model.predict(input_test_AA)\n",
        "accuracy_score(output_pred_AA, output_test_AA)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6777456647398844"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8MotSHTVixD"
      },
      "source": [
        "Now, let's find our **accuracy for only Caucasians!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCeW2jV4VzMg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaee1152-f32a-469b-f186-5dd1e9375244"
      },
      "source": [
        "input_test_C = input_test[input_test['Caucasian'] == 1]\n",
        "output_test_C = output_test[input_test['Caucasian'] == 1]\n",
        "output_pred_C = model.predict(input_test_C)\n",
        "accuracy_score(output_pred_C, output_test_C)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.656319290465632"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCA0a5_rPNx5"
      },
      "source": [
        "**Discuss:** Does our model seem biased, or does it perform about the same on both groups?\n",
        "\n",
        "If it is biased, does the direction of the bias surprise you?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boL5rk2BRbE4"
      },
      "source": [
        "### Comparing Error Types\n",
        "\n",
        "So far, it looks like our model performs about the same on both groups: modestly above random performance (around 65%).\n",
        "\n",
        "Now, let's check out how our model fails when it gets it wrong. There are four possible situtations:\n",
        "- Correct: predicting a person will recidivate when they will (True Positive)\n",
        "- Correct: predicting a person won't recidvate when they won't (True Negative)\n",
        "- Mistake: incorrectly predicting that a person will recidivate when they actually won't (False Positive)\n",
        "- Mistake: incorrectly predicting a person won't recidivate when they will (False Negative)\n",
        "\n",
        "**Discuss:** What are the consequences of each kind of mistake? Which one do you think is worse, and why?\n",
        "\n",
        "Let's check out how often our model makes each kind of mistake!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4LWR-ENZpPg"
      },
      "source": [
        "Let's start with **African Americans**. Here's the breakdown of our model's results:\n",
        "\n",
        "**Check your understanding:** Which cell in the confusion matrix corresponds to a false positive? What about a false negative? Can you find where the true positives and true negatives are as well?\n",
        "\n",
        "**Discuss:** How many people are in each category? Does the model make one kind of mistake more than the other, or is it about equal?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RniZy9GmLtpW"
      },
      "source": [
        "plot_errors(model, input_test_AA, output_test_AA)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4o53B65aYGs"
      },
      "source": [
        "Now, let's try the same thing with **Caucasians!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYuYl8noahd6"
      },
      "source": [
        "plot_errors(model, input_test_C, output_test_C)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84a-12L6aqRz"
      },
      "source": [
        "**Discuss:** For Caucasians, how often does the model make two kinds of mistakes? Is the model \"more forgiving\" or \"less forgiving\" for Caucasians compared to African-Americans? Is it biased?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkgsES_ma905"
      },
      "source": [
        "# (Challenge) Fighting Bias\n",
        "\n",
        "Whoa - our model is biased! Although it has about the same accuracy for both groups, a closer inspection showed that it's much more forgiving for White than Black defendants.\n",
        "\n",
        "**Discuss:** How could this be? How could we fix it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGvfaSI3bNml"
      },
      "source": [
        "### Removing Race\n",
        "\n",
        "Maybe the problem is that we used race as an input to our model! Does that seem right?\n",
        "\n",
        "Let's **try removing race from the model.** We'll take the same train and test splits that we worked with, and drop all features corresponding to race.\n",
        "\n",
        "Let's start by **removing the race columns.**\n",
        "\n",
        "**Hint:** Look at the [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html) of the `drop` function to see how to drop columns from a DataFrame!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Msvja9z8bfCD"
      },
      "source": [
        "race_column_names = [\"African-American\", \"Asian\", \"Caucasian\", \"Hispanic\", \"Native American\", \"Other\"]\n",
        "\n",
        "input_train_no_race = input_train.drop(race_column_names, axis='columns') #remove race columns\n",
        "input_test_no_race = input_test.drop(race_column_names, axis='columns') #remove race columns\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlEawkkeiIuY"
      },
      "source": [
        "Now, we've filled in a cell to retrain a new logistic regression model on this new data! This will print out the accuracies for each group (Black and White) as well as the confusion matrices. All the code is similar to what we've done before!\n",
        "\n",
        "**Discuss:** What do we see in the results? Is this better or worse than the original model, which included racial information?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03rEpRzYiHy6"
      },
      "source": [
        "# Fit a logistic regression model to the new data and report accuracy\n",
        "no_race_model = LogisticRegression(max_iter=1000)\n",
        "no_race_model.fit(input_train_no_race, output_train)\n",
        "preds = no_race_model.predict(input_test_no_race)\n",
        "print(\"Overall accuracy:\", accuracy_score(output_test, preds))\n",
        "\n",
        "# Split up the inputs of the new data by race\n",
        "input_test_AA_no_race = input_test_no_race[input_test['African-American'] == 1]\n",
        "input_test_C_no_race = input_test_no_race[input_test['Caucasian'] == 1]\n",
        "\n",
        "# Split up the predictions generated on the new data by race\n",
        "preds_AA_no_race = preds[input_test['African-American'] == 1]\n",
        "preds_C_no_race = preds[input_test['Caucasian'] == 1]\n",
        "\n",
        "# Report accuracy by group\n",
        "print(\"Accuracy, African-American group\", accuracy_score(output_test_AA, preds_AA_no_race))\n",
        "print(\"Accuracy, Caucasian group\", accuracy_score(output_test_C, preds_C_no_race))\n",
        "print()\n",
        "\n",
        "# Plot confusion matrices\n",
        "plot_errors(no_race_model, input_test_AA_no_race, output_test_AA, title=\"Confusion matrix, Black defendants\")\n",
        "plot_errors(no_race_model, input_test_C_no_race, output_test_C, title=\"Confusion matrix, White defendants\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsJ8DuV0gMsv"
      },
      "source": [
        "### Other Approaches\n",
        "\n",
        "It turns out that removing race does not remove racial bias! How could this be?\n",
        "\n",
        "One possible explanation is that racial bias can be **baked into the data.**\n",
        "\n",
        "For example, Black adults are [much more likely](https://www.aclu.org/gallery/marijuana-arrests-numbers) than White adults to be arrested for marijuana use despite approximately equal usage rates. This means that if someone's arrested for marijuana, they're probably Black - so if a model pays attention to marijuana arrests, it may develop a racial bias!\n",
        "\n",
        "So how do we actually fight AI bias? It's an open research area, so this is where you come in!\n",
        "\n",
        "Let's put on our scientific method hats for a second! Come up with a hypothesis for making your model more fair, and test it out. As a starting point, here's some ideas to ponder. We'll recommend a simple choice:\n",
        "\n",
        "* **Different modeling choices:** Pick a different model from the `scikit-learn` library! You can simply copy-paste your old `LogisticRegression` code, replacing `LogisticRegression` with whatever model you want to try. Here's some suggestions out of the box -- don't worry if you don't know all the details:\n",
        "\n",
        "```\n",
        "Ridge Regression: from sklearn.linear_model import Ridge\n",
        "LASSO Regression: from sklearn.linear_model import Lasso\n",
        "Elastic-Net Regression (combination of Ridge and LASSO): from sklearn.linear_model import ElasticNet\n",
        "Support-Vector Classifiers: from sklearn.svm import SVC\n",
        "K-Nearest Neighbor Classifier: from sklearn.neighbors import KNeighborsClassifier\n",
        "Naive Bayes Classifier: from sklearn.naive_bayes import MultinomialNB\n",
        "```\n",
        "\n",
        "You're encouraged to look up what each one of these classifiers does, using the official `scikit-learn` documentation.\n",
        "\n",
        "You can also find other methods [here](https://scikit-learn.org/stable/supervised_learning.html).\n",
        "\n",
        "#### Optional: Advanced Methods for Algorithmic Fairness\n",
        "\n",
        "As a bonus, feel free to consider these ideas as well! These are much more time-consuming, but can give you some interesting results.\n",
        "\n",
        "* **\"Garbage in, garbage out:\"** if racial bias is inherently baked into this dataset, perhaps changing the model won't help. Can we use sampling to [stratify our sample](https://en.wikipedia.org/wiki/Stratified_sampling), so that we can fairly classify everyone?\n",
        "* **Add more layers:** Deeper models have more representational power, and are able to learn more complex data distributions. Does this translate to a more fair model?\n",
        "* **Decreasing the number of parameters:** Deeper models are a double-edged sword -- they're also known to overfit to spurious correlations in the data, which can exacerbate biases. In this case, does simplifying the model help fairness?\n",
        "* **Group-wise thresholds:** Treating \"like cases alike and unlike cases unlike\" is a possible definition of justice. How do we choose good \"thresholds\" for each group?\n",
        "\n",
        "Feel free to select one of these ideas to test, or come up with your own! For more inspiration, [feel free to look at the Wikipedia page on machine learning fairness.](https://en.wikipedia.org/wiki/Fairness_(machine_learning)) Some additional notes:\n",
        "* You're encouraged to reuse code from above for preprocessing/training models.\n",
        "* You're free to (and even _encouraged_ to) Google a tutorial for implementing a model -- the point is to learn how to analyze model fairness, not measure how well you can hack together a ML pipeline.\n",
        "\n",
        "**It doesn't matter if your model doesn't have good accuracy or good fairness.** The point of this exercise is to do error analysis, not create a perfect model. This isn't an easy problem -- people spend their careers studying this issue!\n",
        "\n",
        "#### Optional Guiding Questions: Towards a Fairer Model\n",
        "\n",
        "To help you think about this problem, here's a few guiding questions to think about. No need to write anything down, this is to help you think!\n",
        "\n",
        "* Why do you think our old model was unfair?\n",
        "* Why do you think your proposed method might overcome these biases? There's no wrong answer -- you're just coming up with a reasonable hypothesis right now, and you can always refine it as you get more information.\n",
        "* How do you plan on evaluating the fairness of the model? Is it the same setup as above?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMIrzR8IKUjK"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "**Congratulations!** In this notebook, you've built a version of COMPAS and discovered bias in it through close inspection.\n",
        "\n",
        "**What conclusions do you draw?**\n",
        "- Would you recommend this model to a judge, even though it's biased, if it might be *less* biased than a human?\n",
        "- In other situations, do you think AI in the criminal justice system is a good idea? Why or why not?\n",
        "- Is is possible to create a truly unbiased dataset? Are synthetic datasets (data not from real people, but generated statistically) unbiased?\n",
        "\n",
        "## Optional: Some Further Thoughts\n",
        "\n",
        "Unlike other ML tasks, creating a fair ML system is a problem without a clear-cut solution. We hope this has made you stop and think about how AI is deployed in the real world, and the difficulty of developing \"fair\" AI systems.\n",
        "\n",
        "It's easy to focus on accuracy and improving performance -- these are certainly important! However, thinking not just about _fairness_, but the broad impact they have on real peoples' lives is just as -- and sometimes even more -- important. In some cases, treating people fairly and improving your model performance contradict one another! That's what makes thinking about AI fairness in the real world so difficult: there's multiple tradeoffs to consider.\n",
        "\n",
        "With the increasing adoption of machine learning systems, we encourage you to think _beyond_ the mathematical fairness or performance of a model. For example:\n",
        "* **Beyond Accuracy:** Even if a model is 100% accurate, who does that benefit? In your opinion, is the world a \"better\" place now?\n",
        "* **Differing Definitions of Fairness:** We've given you _one_ definition of fairness to work with. Are there other ways we can define if a model is fair? If two definitions of fairness contradict one another -- how would you evaluate the model in that case?\n",
        "* **Dataset Creation and Power Dynamics:** Where is the data coming from, and who's labeling the data? For this dataset, each individual 1) encountered the police, 2) was arrested, and lastly 3) a decision was made about the specific charge and its severity. Where might societal biases and inequalities seep into this process, if at all?\n",
        "* **Fairness in the Wild:** When we deploy a machine learning model in the real world, how can we identify when the model is being \"unfair,\" and how can we address these problems?\n",
        "* **Limitataions of AI:** What problems do you think that AI inherently cannot or should not solve?\n",
        "\n",
        "Lastly, we'll leave with an impactful quote:\n",
        "\n",
        "> \"All models are wrong. Some are useful. Most are anti-Black.\"\n",
        "\n",
        "_-- [Devin Guillory](http://www.devinguillory.com/), PhD, Berkeley EECS; talk on [Combatting Anti-Blackness in the AI Community (The Ethics of AI in Context)](https://www.youtube.com/watch?v=2M3QSQ-N3pQ&ab_channel=CentreforEthics), Feb. 2021._\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMQjeIHo3Jm9"
      },
      "source": [
        "# Further Reading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DkQgW9V3Tsz"
      },
      "source": [
        "## Technical\n",
        "\n",
        "Angwin, Julia et. al. \"Machine Bias,\" *ProPublica,* May 23, 2016, https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing\n",
        "\n",
        "Bolukbasi, Tolga, et al. \"Man is to computer programmer as woman is to homemaker? debiasing word embeddings.\" arXiv preprint arXiv:1607.06520 (2016).\n",
        "\n",
        "Buolamwini, Joy, and Timnit Gebru. \"Gender shades: Intersectional accuracy disparities in commercial gender classification.\" Conference on fairness, accountability and transparency. PMLR, 2018.\n",
        "\n",
        "Gebru, Timnit, et al. \"Datasheets for datasets.\" arXiv preprint arXiv:1803.09010 (2018).\n",
        "\n",
        "Mitchell, Margaret, et al. \"Model cards for model reporting.\" Proceedings of the conference on fairness, accountability, and transparency. 2019."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_eJvyCj3Tw0"
      },
      "source": [
        "## Non-technical\n",
        "\n",
        "Alexander, Michelle. \"The New Jim Crow: Mass Incarceration in the Age of Colorblindness,\" New York, NY: The New Press, 2010.\n",
        "\n",
        "Barocas, Solon, and Andrew D. Selbst. \"Big data's disparate impact.\" Calif. L. Rev. 104 (2016): 671.\n",
        "\n",
        "Benjamin, Ruha. \"Race after technology: Abolitionist tools for the new jim code.\" Social Forces (2019).\n",
        "\n",
        "Broussard, Meredith. Artificial unintelligence: How computers misunderstand the world. MIT Press, 2018.\n",
        "\n",
        "Christin, Angèle. “Algorithms in Practice: Comparing Web Journalism and Criminal Justice.” Big Data & Society, December 2017. doi:10.1177/2053951717718855.\n",
        "\n",
        "Kaminski, Margot E., and Gianclaudio Malgieri. \"Algorithmic impact assessments under the GDPR: producing multi-layered explanations.\" U of Colorado Law Legal Studies Research Paper 19-28 (2019).\n",
        "\n",
        "O'Neil, Cathy. \"Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy,\" Largo, MD: Crown Publishing Group, 2017."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VnsBOUP3OJf"
      },
      "source": [
        "## More Advanced Resources\n",
        "\n",
        "* ACM FAccT Conference website: https://facctconference.org/\n",
        "* Stanford CS335 (Fair, Accountable, and Transparent Machine Learning): https://hci.stanford.edu/courses/cs335/2020/sp/\n",
        "* Stanford CS384 (Ethical and Social Issues in Natural Language Processing): https://web.stanford.edu/class/cs384/\n",
        "* Fair Machine Learning Textbook: https://fairmlbook.org -- Ch. 2 dives deeper into COMPAS and mathematical fairness!"
      ]
    }
  ]
}